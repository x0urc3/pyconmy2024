{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eba8e38d-0344-4b80-b5dd-e50e662bb0c3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Gearing up Python for High-Speed HDF Adventures\n",
    "## Khairulmizam Samsudin, PhD\n",
    "#### Faculty of Engineering, Universiti Putra Malaysia\n",
    "#### khairulmizam@upm.edu.my"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa07f88-2d36-4eb2-88d8-c68de34ac3bc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd85170a-9360-43f2-a461-6648002b7d24",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate diagram for Data Flow slide\n",
    "graph = \"\"\"\n",
    "sequenceDiagram\n",
    "    participant Sensor as Sensor\n",
    "    participant LocalStorage as Local Storage (HDF5)\n",
    "    participant Preprocessing as Preprocessing\n",
    "    participant AISystem as AI System\n",
    "    participant Decision as Decision/Action\n",
    "\n",
    "    Sensor->>LocalStorage: Collects Raw Data\n",
    "    LocalStorage->>Preprocessing: Stores Raw Data\n",
    "    Preprocessing->>LocalStorage: Processes and Saves Clean Data\n",
    "    LocalStorage->>AISystem: Provides Clean Data\n",
    "    MLModel->>Decision: Predicts/Analyzes Data\n",
    "    Decision-->>AISystem: Feedback for System Improvement\n",
    "    AISystem-->>Preprocessing: Requests More Data (if needed)\n",
    "\"\"\"\n",
    "mermaid_ink(graph, \"images/context1.jpg\")\n",
    "\n",
    "graph = \"\"\"\n",
    "sequenceDiagram\n",
    "    box white Focus of this talk\n",
    "    participant Sensor as Sensor\n",
    "    participant LocalStorage as Local Storage (HDF5)\n",
    "    participant Preprocessing as Preprocessing\n",
    "    end\n",
    "    participant AISystem as AI System\n",
    "    participant Decision as Decision/Action\n",
    "\n",
    "    Sensor->>LocalStorage: Collects Raw Data\n",
    "    LocalStorage->>Preprocessing: Stores Raw Data\n",
    "    Note over Sensor, LocalStorage: Focus of this talk\n",
    "    Preprocessing->>LocalStorage: Processes and Saves Clean Data\n",
    "    LocalStorage->>AISystem: Provides Clean Data\n",
    "    MLModel->>Decision: Predicts/Analyzes Data\n",
    "    Decision-->>AISystem: Feedback for System Improvement\n",
    "    AISystem-->>Preprocessing: Requests More Data (if needed)\n",
    "\"\"\"\n",
    "#mermaid_ink(graph)\n",
    "mermaid_ink(graph, \"images/context1a.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84758159-7a26-408d-a3b1-e05950cd4e65",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Data Flow\n",
    "![Context of HDF5](./images/context1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26da5f03-92c4-4e35-9d42-33e86e1dcaf2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Data Flow\n",
    "![Context of HDF5](./images/context1a.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec51c18b-4d95-447b-a919-4c4e14d905b2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Background\n",
    "- Focus on optimizing data storage to handle large datasets efficiently and at high speeds during data acquisition stage\n",
    "- **Data acquisition** is the crucial first step in AI and engineering applications.\n",
    "  - The quality and efficiency of data collection directly impact the performance and accuracy of AI/ML models\n",
    "  - Challenges: size, bandwidth, latency and cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06075f8-3bcf-4d09-bc33-799a245bd672",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Introduction to HDF5\n",
    "- **Overview**:\n",
    "  - [HDF5](https://www.hdfgroup.org/) is an open file format and set of tools for managing complex data.\n",
    "  - Ideal for storing large volumes of numerical data efficiently. (e.g. MRI, LIDAR point cloud, genomic, geospatial, earth science) \n",
    "  - Supported programming languages: **C**, C++, Fortran, **Python**, Java and many more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0825ea59-2f32-46b3-a441-f2c948066648",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Why HDF5?\n",
    "- **Performance**: Faster read/write speeds compared to traditional formats like CSV\n",
    "- **Scalability**: Capable of handling large datasets without significant performance degradation.\n",
    "- **Flexibility**: Supports a variety of data types and complex data structures. i.e. metadata and data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec91b764-ec64-470b-aa6f-a6075dc1d5ac",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Limitations of HDF5\n",
    "1. **Inflexibility of Datasets**: Once a dataset is written in HDF5, it cannot be directly overwritten.\n",
    "2. **Non-Extendable Datasets**:By default, datasets in HDF5 are of a fixed size, meaning they cannot grow beyond their initial allocation.\n",
    "3. **Fragmentation Issues**: Appending datasets can lead to fragmentation, where the file system has to manage scattered blocks of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb87505-2dc0-4334-9ef9-22e97459a252",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Demo setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2e1a48d-7802-4c0a-b9b2-e365bbe8e7e1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "%load_ext memory_profiler\n",
    "\n",
    "# Generate sample data\n",
    "large_data = np.random.rand(int(1e5), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aba464-d0ff-418e-803e-080282d53a95",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### HDF5 vs CSV Read/Write performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b840077d-3e9c-4ed9-b202-2eedbad8bf05",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling Write\n",
      "1.21 s ± 84.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "72.2 ms ± 18.6 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Profiling Read\n",
      "239 ms ± 1.34 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "10.6 ms ± 44.1 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "def csv_write(filename, data):\n",
    "    with open(filename, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(data)\n",
    "def h5_write(filename, data, dtype = 'float64', chunk_size = True):\n",
    "    with h5py.File(filename, 'w') as file:\n",
    "        file.create_dataset('dataset', data=data.astype(dtype), chunks = chunk_size)    \n",
    "def csv_read(filename):\n",
    "    with open('sample_data.csv', 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        data_csv = [row for row in reader]\n",
    "def h5_read(filename ):\n",
    "    with h5py.File('sample_data.h5', 'r') as file:\n",
    "        data_hdf5 = file['dataset'][:]\n",
    "\n",
    "print(f\"Profiling Write\")\n",
    "%timeit csv_write('sample_data.csv', large_data)\n",
    "%timeit h5_write('sample_data.h5', large_data)\n",
    "print(f\"Profiling Read\")\n",
    "%timeit csv_read('sample_data.csv')\n",
    "%timeit h5_read('sample_data.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1393491e-614e-4e32-b27f-bc0957244644",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### HDF5 vs CSV Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3518dc43-34d1-4700-8d02-7e21ad8ad4ee",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV File Size: 18.47 MB\n",
      "HDF5 File Size: 7.63 MB\n"
     ]
    }
   ],
   "source": [
    "csv_size = os.path.getsize('sample_data.csv')\n",
    "hdf5_size = os.path.getsize('sample_data.h5')\n",
    "\n",
    "print(f\"CSV File Size: {csv_size / 1024**2:.2f} MB\")\n",
    "print(f\"HDF5 File Size: {hdf5_size / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c615ff5c-a8d7-48be-b4cd-6e2173e243d3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Alternatives\n",
    "\n",
    "| Feature | **h5py** | **PyTables** | **Tiled** | **Parquet** | **Feather** | **CSV** |\n",
    "| :-- | :-: | :-: | :-: | :-: | :-: | :-: |\n",
    "| **Hierarchical Data**              | X | X | X | X |  |  |\n",
    "| **Dense Data**                     | X | X | X | X | X |  |\n",
    "| **Sparse Data**                    | ~ | X | X | X | X |  |\n",
    "| **Compression**                    | X | X | X | X | X |  |\n",
    "| **Data Querying**                  |   | X | X | X |   |  |\n",
    "| **Parallel I/O**                   | X | X | X | X |   |  |\n",
    "| **Hardware Interface (C/C++)**     | X | X |   |  |   |  |\n",
    "| **Portability** | X | X  |  |   |  |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef6ac60-50f9-43fb-a46c-2a2737c77fc8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Best Practices (BP) for Using HDF5\n",
    "   1. Use the Right Data Type\n",
    "   2. Store Data Incrementally\n",
    "   3. Proper Chunking\n",
    "   4. Enable Compression\n",
    "   5. Optimize Access Patterns\n",
    "   9. Leverage Virtual Datasets (VDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcba3113-de2d-4a73-b9ac-a8c8f7967618",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### [BP-1] Use the Right Data Type\n",
    "   - Choose the smallest suitable data type that can accurately represent your data.\n",
    "   - Avoid excessive precision (e.g. `float64` vs `float32` vs `float16`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "13dda9d7-b737-486e-a100-55c98b09da80",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing RGB8 as int64\n",
      "61.9 ms ± 11.6 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Writing RGB8 as int8\n",
      "13.3 ms ± 6.25 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "HDF5 float64 File Size: 7.63 MB\n",
      "HDF5 int8 File Size: 0.96 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Writing RGB8 as int64\")\n",
    "%timeit h5_write('sample_data_int64.h5', large_data, dtype = 'int64')\n",
    "print(f\"Writing RGB8 as int8\")\n",
    "%timeit h5_write('sample_data_int8.h5', large_data, dtype = 'int8')\n",
    "\n",
    "hdf5_int64_size = os.path.getsize('sample_data_int64.h5')\n",
    "print(f\"HDF5 float64 File Size: {hdf5_float64_size / 1024**2:.2f} MB\")\n",
    "hdf5_int8_size = os.path.getsize('sample_data_int8.h5')\n",
    "print(f\"HDF5 int8 File Size: {hdf5_int8_size / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633c5f4b-a4c2-49c7-abea-b69db89d3e6e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### [BP-2] Store Data Incrementally\n",
    "   - Avoid keeping everything in memory\n",
    "   - Write data to the HDF5 file incrementally rather than accumulating everything in memory and writing it all at once.\n",
    "   - This is particularly important for large datasets that might not fit into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbde8d4-6e82-41e0-b68a-5809ee391461",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### [BP-3] Proper Chunking\n",
    "   - Choose chunk sizes that match typical access pattern to minimize I/O\n",
    "   - Find a balance that works for your specific use case by testing different chunk sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "be2ad040-ad83-44e8-ae69-9640cb106ed3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sample_data_chunk_10x10.h5\n",
      "117 ms ± 7.15 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Writing sample_data_chunk_1000x10.h5\n",
      "54.3 ms ± 2.95 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Writing sample_data_chunk_10000x10.h5\n",
      "56.1 ms ± 4.57 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "chunk_sizes = [(10, 10), (1000, 10), (10000, 10)]\n",
    "for chunk_size in chunk_sizes:\n",
    "    filename = f'sample_data_chunk_{chunk_size[0]}x{chunk_size[1]}.h5'\n",
    "    print(f\"Writing {filename}\")\n",
    "    %timeit h5_write(filename, large_data, chunk_size = chunk_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae632deb-c0b5-4a85-be13-49d79d4e05cd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### [BP-4] Enable Compression\n",
    "   - Use compression filters like `gzip`, `lzf`, or `szip` to reduce file size. `gzip` is a good default choice that balances speed and compression ratio.\n",
    "   - Compression is more effective on data with redundancy (e.g. sparse matrices, repeated values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96855251-f4f4-490f-be2b-a1112b449ac7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### [BP-5] Optimize Access Patterns\n",
    "   - Sequential access is faster than random access. Structure your data and access patterns to read and write data sequentially whenever possible.\n",
    "   - Store related data in the same chunk or group to reduce the number of I/O operations needed to access the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cc30ea-3039-4cf3-8023-7c0d6628a424",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Common Mistakes\n",
    "   - Assuming default settings are optimal. Always evaluate and adjust.\n",
    "   - Large or complex metadata stored as attributes can degrade performance. \n",
    "   - Compression reduces file size but can add overhead to I/O operations. It’s important to test if the benefits outweigh the costs in terms of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2d4747-ddc0-4120-a7e5-e5c30db91445",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Conclusion\n",
    "   - HDF5 is ideal for during data-intensive AI and engineering applications\n",
    "   - HDF5 is effective for handling large, complex datasets.\n",
    "   - Proper data type selection, incremental storage, and chunking are essential.\n",
    "   - Optimization ensures efficiency in speed, memory, and storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdfebcf-dcd8-4d8a-ba86-2020275a635c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Thank You\n",
    "https://github.com/x0urc3/talks/pyconmy2024"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
